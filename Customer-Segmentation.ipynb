{"cells":[{"cell_type":"code","source":["import os\nimport warnings\nwarnings.simplefilter(action = 'ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\ndef ignore_warn(*args, **kwargs):\n    pass\n\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\nimport pandas as pd\nimport datetime\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport matplotlib.cm as cm\n\n%matplotlib inline\n\nfrom pandasql import sqldf\npysqldf = lambda q: sqldf(q, globals())\n\nimport seaborn as sns\nsns.set(style=\"ticks\", color_codes=True, font_scale=1.5)\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\nimport plotly as py\nimport plotly.graph_objs as go\npy.offline.init_notebook_mode()\n\nfrom scipy import stats\nfrom scipy.stats import skew, norm, probplot, boxcox\nfrom sklearn import preprocessing\nimport math\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport Orange\nfrom Orange.data import Domain, DiscreteVariable, ContinuousVariable\nfrom orangecontrib.associate.fpgrowth import *"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/data.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["cs_df = df\n# Remove register withou CustomerID\n#cs_df = cs_df[~(cs_df.CustomerID.isnull())]\ncs_df.filter(cs_df.CustomerID.isNotNull())\n\n# Remove negative or return transactions\ncs_df = cs_df[~(cs_df.Quantity<0)]\ncs_df = cs_df[cs_df.UnitPrice>0]\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["#refrence_date = cs_df.InvoiceDate.max() + datetime.timedelta(days = 1)\nfrom pyspark.sql.functions import datediff, to_date, lit\nfrom pyspark.sql.types import StringType, IntegerType, StructType, StructField\nfrom pyspark.sql.functions import udf\nimport pyspark.sql.functions as F\nfrom pyspark.sql.functions import col\nmax_date = cs_df.agg({'InvoiceDate': 'max'}).collect()[0]\nmax_date = max_date[\"max(InvoiceDate)\"]\nprint('Max Date:', max_date)\ncs_dfs = cs_df.withColumn(\"days_since_last_purchase\", datediff(lit(max_date),cs_df.InvoiceDate))#.show(truncate=False)\n\n#cs_dfs = cs_df.withColumn('days_since_last_purchase', F.datediff(F.to_date(F.date_add(F.to_date(lit(max_date)),1)), F.to_date(cs_df.InvoiceDate)))\n#cs_dfs.filter(cs_dfs.days_since_last_purchase.isNotNull())\n\n#cs_dfs.show()\ncustomer_history_df =  cs_dfs[['CustomerID', 'days_since_last_purchase']].groupby(\"CustomerID\").count()\ncustomer_history_df.orderBy(customer_history_df[\"count\"].desc())\ncustomer_history_df = customer_history_df.select(col(\"CustomerID\").alias(\"CustomerID\"),col(\"count\").alias(\"recency\"))\ncustomer_history_df.show()\n#customer_history_df.describe().transpose()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Max Date: 9/9/2011 9:52\n+----------+-------+\nCustomerID|recency|\n+----------+-------+\n     16250|     18|\n     15574|    139|\n     15555|    534|\n     15271|    211|\n     17714|     10|\n     17686|    259|\n     13865|     29|\n     14157|     38|\n     13610|    183|\n     13772|    135|\n     13282|     35|\n     12394|     23|\n     16320|     38|\n     13192|     47|\n     14887|      5|\n     17506|     11|\n     17427|      2|\n     18130|     46|\n     16504|     41|\n     15269|      2|\n+----------+-------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["customer_freq = (cs_df[['CustomerID', 'InvoiceNo']].groupby([\"CustomerID\", 'InvoiceNo']).count().groupby([\"CustomerID\"]).count())\ncustomer_history_df = customer_history_df.join(customer_freq, on=['CustomerID'], how='left_outer')\ncustomer_history_df = customer_history_df.select(col(\"CustomerID\").alias(\"CustomerID\"),col(\"recency\").alias(\"recency\"),col(\"count\").alias(\"frequency\"))\ncustomer_history_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------+---------+\nCustomerID|recency|frequency|\n+----------+-------+---------+\n     16250|     18|        2|\n     15574|    139|        4|\n     15555|    534|       16|\n     15271|    211|       15|\n     17714|     10|        1|\n     17686|    259|        6|\n     13865|     29|        4|\n     14157|     38|        2|\n     13610|    183|        7|\n     13772|    135|        3|\n     13282|     35|        3|\n     12394|     23|        2|\n     16320|     38|        2|\n     13192|     47|        2|\n     14887|      5|        1|\n     17506|     11|        1|\n     17427|      2|        1|\n     18130|     46|        3|\n     16504|     41|        1|\n     15269|      2|        1|\n+----------+-------+---------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["from pyspark.sql.functions import col\ncs_df_amount = cs_df.withColumn(\"amount\",col(\"Quantity\") * col(\"UnitPrice\"))#.groupby(\"CustomerID\").sum()\\\n#(cs_df[['CustomerID', 'InvoiceNo']].groupby([\"CustomerID\", 'InvoiceNo']).count().groupby([\"CustomerID\"]).count())\ncustomer_monetary_val = (cs_df_amount[['CustomerID', 'amount']].groupby([\"CustomerID\"]).sum())\ncustomer_history_df = customer_history_df.join(customer_monetary_val, on=['CustomerID'], how='left_outer')\ncustomer_history_df = customer_history_df.select(col(\"CustomerID\").alias(\"CustomerID\"),col(\"recency\").alias(\"recency\"),col(\"frequency\").alias(\"frequency\"),col(\"sum(amount)\").alias(\"tot_amount\"))\ncustomer_history_df.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------+---------+------------------+\nCustomerID|recency|frequency|        tot_amount|\n+----------+-------+---------+------------------+\n     16250|     18|        2|             319.5|\n     15574|    139|        4| 642.6700000000001|\n     15555|    534|       16|3541.1100000000006|\n     15271|    211|       15|2237.1299999999997|\n     17714|     10|        1|             153.0|\n     17686|    259|        6|           5452.28|\n     13865|     29|        4|494.96000000000004|\n     14157|     38|        2| 363.9200000000001|\n     13610|    183|        7|1068.0599999999997|\n     13772|    135|        3|1011.7400000000002|\n     13282|     35|        3|1111.6799999999998|\n     12394|     23|        2|           1034.72|\n     16320|     38|        2|             840.1|\n     13192|     47|        2| 775.3200000000002|\n     14887|      5|        1|            1790.0|\n     17506|     11|        1|             270.7|\n     17427|      2|        1|             100.8|\n     18130|     46|        3| 799.8499999999999|\n     16504|     41|        1|315.44999999999993|\n     15269|      2|        1|             408.8|\n+----------+-------+---------+------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import col\ncustomer_history_df = customer_history_df.withColumn(\"recency_log\", F.log(col(\"recency\")))\ncustomer_history_df = customer_history_df.withColumn(\"freq_log\", F.log(col(\"frequency\")))\ncustomer_history_df = customer_history_df.withColumn(\"amt_log\", F.log(col(\"tot_amount\")))\nfeature_vector = ['amt_log', 'recency_log','freq_log']\nX_subset = customer_history_df[feature_vector] #.as_matrix()\nscaler = preprocessing.StandardScaler().fit(X_subset)\nX_scaled = scaler.transform(X_subset)\npd.DataFrame(X_scaled, columns=X_subset.columns).describe().T"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2255571884068052&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      5</span> feature_vector <span class=\"ansi-blue-fg\">=</span> <span class=\"ansi-blue-fg\">[</span><span class=\"ansi-blue-fg\">&#39;amt_log&#39;</span><span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">&#39;recency_log&#39;</span><span class=\"ansi-blue-fg\">,</span><span class=\"ansi-blue-fg\">&#39;freq_log&#39;</span><span class=\"ansi-blue-fg\">]</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      6</span> X_subset <span class=\"ansi-blue-fg\">=</span> customer_history_df<span class=\"ansi-blue-fg\">[</span>feature_vector<span class=\"ansi-blue-fg\">]</span> <span class=\"ansi-red-fg\">#.as_matrix()</span>\n<span class=\"ansi-green-fg\">----&gt; 7</span><span class=\"ansi-red-fg\"> </span>scaler <span class=\"ansi-blue-fg\">=</span> preprocessing<span class=\"ansi-blue-fg\">.</span>StandardScaler<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>fit<span class=\"ansi-blue-fg\">(</span>X_subset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      8</span> X_scaled <span class=\"ansi-blue-fg\">=</span> scaler<span class=\"ansi-blue-fg\">.</span>transform<span class=\"ansi-blue-fg\">(</span>X_subset<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">      9</span> pd<span class=\"ansi-blue-fg\">.</span>DataFrame<span class=\"ansi-blue-fg\">(</span>X_scaled<span class=\"ansi-blue-fg\">,</span> columns<span class=\"ansi-blue-fg\">=</span>X_subset<span class=\"ansi-blue-fg\">.</span>columns<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>describe<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>T\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/sklearn/preprocessing/data.py</span> in <span class=\"ansi-cyan-fg\">fit</span><span class=\"ansi-blue-fg\">(self, X, y)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/sklearn/preprocessing/data.py</span> in <span class=\"ansi-cyan-fg\">partial_fit</span><span class=\"ansi-blue-fg\">(self, X, y)</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/sklearn/utils/validation.py</span> in <span class=\"ansi-cyan-fg\">check_array</span><span class=\"ansi-blue-fg\">(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    525</span>                                         UInt8Dtype<span class=\"ansi-blue-fg\">,</span> UInt16Dtype<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    526</span>                                         UInt32Dtype, UInt64Dtype)\n<span class=\"ansi-green-fg\">--&gt; 527</span><span class=\"ansi-red-fg\">                     if isinstance(dtype_iter, (Int8Dtype, Int16Dtype,\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    528</span>                                                Int32Dtype<span class=\"ansi-blue-fg\">,</span> Int64Dtype<span class=\"ansi-blue-fg\">,</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    529</span>                                                UInt8Dtype<span class=\"ansi-blue-fg\">,</span> UInt16Dtype<span class=\"ansi-blue-fg\">,</span>\n\n<span class=\"ansi-green-fg\">/databricks/python/lib/python3.7/site-packages/numpy/core/numeric.py</span> in <span class=\"ansi-cyan-fg\">asarray</span><span class=\"ansi-blue-fg\">(a, dtype, order)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    536</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    537</span>     &#34;&#34;&#34;\n<span class=\"ansi-green-fg\">--&gt; 538</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">return</span> array<span class=\"ansi-blue-fg\">(</span>a<span class=\"ansi-blue-fg\">,</span> dtype<span class=\"ansi-blue-fg\">,</span> copy<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-green-fg\">False</span><span class=\"ansi-blue-fg\">,</span> order<span class=\"ansi-blue-fg\">=</span>order<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    539</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">    540</span> \n\n<span class=\"ansi-red-fg\">ValueError</span>: setting an array element with a sequence.</div>"]}}],"execution_count":7}],"metadata":{"name":"Customer-Segmentation","notebookId":3340163944242886},"nbformat":4,"nbformat_minor":0}
